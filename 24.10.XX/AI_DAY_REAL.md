## AI 파운데이션 모델 및 기초지식

### 1차시 : AI 소개와 그 구성 요소
 - AI의 구분
    * 인공지능 : 인간의 지능을 모방하여 문제해결, 학습, 추론 등의 작업을 수행하는 시스템 및 기술
    * 머신러닝 : 명시적인 프로그래밍 없이 패턴을 학습하는 방법론
    * 딥러닝 : 인공신경망(뉴럴넷)을 모델로 기계학습하는 것
    * 인공지능 > 머신러닝 > 딥러닝
 
#### 기계학습의 개념
 - 고전적인 기계학습 시스템 처리 과정(추론 과정; Inference)
    1. 센싱 : 실제 세계를 디지털 데이터로 확보
    2. 전처리 : 데이터에서 통계적 패턴을 잡기 쉽도록 하는 밑작업(ex.밝기 균등화)
    3. 특징 추출 : 인식하고자 하는 목표와 연관이 많은 특징들을 추출
    4. 분류 : 기계학습 모델을 통해 입력 특징을 인식 결과로 변환

 - 기계학습 예제
    1. 데이터 수집
        * 샘플
        * 훈련 집합(학습 데이터셋), 테스트 집합(테스트 데이터셋)
        * '고품질' 데이터의 '많은' 확보가 매우 중요 ( data -> label)
    2. 특징 정의 및 추출
        ex) 8x8 화소를 64차원 특징 벡터로 1차원화, 각 축의 화소 비율 등등.. -> 난이도가 너무 높음
    
    3. 분류
        * 모델(아키텍쳐) 선택 & 학습
        * 차원에 따라 : 결정 직선(곡선), 결정 평면(곡면), 결정 초평면(초곡면)

    4. 평가
        * 평가 데이터 : 학습 셋에서 학습 후 테스트 셋에서 평가
        * 평가 기준 : 오류율, 인식률 등...
        * 궁극적 목표 : 과적합 피하기
    
 - 세부 절차들은 모두 수학적(Software)으로 모델링 되어 있음
    * AI는 수학을 많이 사용 : 선형대수, 미분, 최적화
        * Part 1, Mathmatics for Machine Learning by Marc Peter, 2020(Free PDF)
        * 오일석, 기계학습, 한빛아카데미, 2017 (패턴인식, 2023)

 - 최근의 AI는 대부분 딥러닝을 의미
    * 전통적 기계 학습 : 입력 -> 전처리 -> 특징 추출 -> 분류 -> 결과
    * 딥러닝 기반 학습 : 입력 -> 딥뉴럴넷(특징 추출+분류) -> 결과(End2End 학습)

 - 딥러닝의 주요 구성 요소
    * 모델 :입력 데이터를 원하는 출력을 변환해주는 함수
    * 손실 함수 : 모델의 출력을 통해 모델이 잘하고 있는지 판단해 수치화하는 함수
    * 데이터 : 모델이 학습할 데이터
    * 알고리즘 : 손실함수를 줄여주기 위해 모델을 업데이트(최적화, 옵티마이저)

#### AI의 디자인 요소 3(4)가지(데이터까지 4)
##### 모델
 - Classifier : 입력을 클래스로 매핑해주는 함수 in 영상인식
 - 선형 모델
    * 단층 레이어 신경망
        : 뉴런의 신호 전달에서 착안한 초기 인공 신경망, 다수의 입력신호를 선형 결합으로 구성된 연산을 거치고 하나의 결과를 내는 알고리즘
            선형 결합 : 행렬의 곱과 합으로 구성된 연산

    * 모델의 구성
        - 모델을 통해 예측하고자 하는 결과 y
        - 모델 구성하는 가중치 W
        - 입력에 해당하는 데이터 x
        - y= W^T *x + b
 
##### 손실함수 
 - 손실함수 : 모델의 출력값과 실제 데이터 정답 사이의 오차를 계산하는 함수
    * 분류(classification) : 주어진 데이터가 어떤 범주(카테고리)에 속하는지 판단하는 태스크
        - Logit : 표준화되지 않은 날 것 그대로의 모델 예측값
        - 확률 : logit + softmax => 0-1 사이의 확률값으로 나타낸 예측값
        - Cross entropy : 분류에 쓰이는 가장 대표적인 손실 함수(이진 분류 / 다중 분류)
    * 회귀(regression) : 범주가 아닌 연속적인 수치를 예측하는 태스크
        - Mean Squared Error(MSE) - L2 : 제곱하는 방식, 에러가 크면 클수록 치명적인 패널티
        - Mean Absolute Error(MAE) - L1 : 절대값, 모든 에러에 대해 동일한 패널티
    * 파이토치에 손실함수가 구현되어 있음(분류 회귀 모두)
    * 기타 : Triplet, KL Divergence, Smooth L1
            nn.L1Loss, nn.MSELoss, nn.CrossEntropyloss, nn.CTCLoss, nn.NLLLoss

##### 알고리즘
 - 목적 : 예측과 정답 사이의 차이를 최소화 / min err[Label,f(data)]
 - Gradient Descent(GD) : 손실함수의 최소값을 찾아가기 위한 방법(산에서 한발자국씩 고도 낮은곳 찾기)
    * 함수 미분을 통해 기울기 구하고 loss 감소하는 방향으로 weight(parameter) 업데이트, 함수의 최저점까지 반복
 - GD Methods : Batch, Stochastic, Mini-batch
    * BGD : 데이터 전체를 모두 본 다음, 각 데이터의 gradient를 모두 평균내서 업데이트
                문제점 : 데이터셋의 규모에 한계가 생김

    * SGD : 데이터 샘플 1개마다 계산하고 업데이트
                문제점 : noise 영향이 매우 커짐

    * MGD : 샘플을 batch 단위로 묶고, batch마다 계산 후 업데이트
        - Step : 신경망이 학습한 mini-batch 개수
        - Epoch : mini-batch를 모두 학습한 횟수(=전체 데이터셋을 학습한 횟수)

    * Optimizer : GD를 통해 최적의 값을 찾아주는 도구
        - 종류 : SGD, Momentum, NAG, Adagrad, Adadelta, RMSprop, Adam --> 1차 미분

#### 모델 평가의 중요성
 - 학습된 모델을 테스트함으로 일반화 능력을 측정, 이를 통해 모델의 성능 개선 방향 결정
 - 일반화 성능평가(=과적합)를 위해 데이터 분할 필요
 - Overfitting : 학습된 모델이 새로운 데이터에 대한 일반화 성능이 떨어지는 현상
    * 훈련셋에서 높은 성능, 테스트셋에서 낮은 성능
    * 대표적 Overfitting 치료법 : Data augmentation, Regularization, Model design change, More training data

### 2차시 : 간단한 뉴럴넷 모델
#### 선형 모델 및 NLP
 - 선형 모델의 한계
    * 선입출력 사이의 선형 관계만 학습 가능
    * 직선으로 구분되는 문제만 해결 가능, XOR 분포가 한계의 대표적인 예시(오류 없이 구분 불가)
 - 비선형성(Non-Linearity) | Regression case
    * 복잡한 문제 해결 : 현실의 데이터는 비선형적->복잡한 패턴 학습을 위해서 도입
    * 비선형성 없는 선형 모델은 다차원 공간 데이터나 복잡한 분포를 효과적으로 학습 불가
##### 비선형 모델
 - 다층 레이어 신경망(MLP)
    * 딥러닝의 가장 기본적 형태, 여러층의 노드가 연결된 구조의 모델
    * 입력 레이어 - 2개 이상의 은닉 레이어 - 출력 레이어 : 여러 층으로 더욱 복잡한 연산 수행 가능
 - MLP의 구성
    * 행렬 곱셈 : 신경망에서 각 층의 노드 간 연결을 계산하는 기본 연ㅅ나 Y =XW + B
                각 입력값에 가중치를 곱해 다음 층으로 전달
    * 입력층 : 입력 데이터를 받는 층
    * 은닉층 : 가중치와 활성화 함수를 통해 데이터를 처리,가공하여 중요한 특징을 추출하는 역할
    * 출력층 : 최종 결과를 출력
 - 선형 결합 : 선형 결합만으로 이루어진 연산은 아무리 연산을 해도 결국 선형 결합
 - 비선형성 사용하기 : 활성화 함수
    * 활성화 함수 역할 : 각 뉴런의 출력을 비선형으로 변환하여 복잡한 관계 학습 가능케 함
 - 주요 활성화 함수
    * ReLU : 0보다 작으면 0, 0보다 크면 그대로 출력 / 가장 많이 사용되는 활성화 함수
        - 장점 : 계산이 간단하고 Vanishing Gradient 문제를 해결
        - 단점 : 0 이하로 학습이 멈추는 dying ReLU 문제 발생 가능
    * Sigmoid : 출력 값을 0과 1 사이로 제한 / 주로 이진 분류 문제에서 사용, 출력이 확률처럼 해석됨
                문제점 : 입력이 너무 크거나 작으면, 경사 하강법에서 기울기가 거의 0이 됨 -> 학습이 어려워짐(Vanishing Gradient Problem)
    * Swish : 최근에 제안된 활성화 함수, ReLU와 Sigmoid의 조합, 출력이 부드럽게 변화
                특정 상황에서 ReLU보다 더 나은 성능을 보임

#### 선형 모델 Revisit
##### 선형 분류기
 - CIFAR10 : 10개의 class가 있는 이미지 데이터셋
 - f(x,W)=Wx + b
 - PyTorch flatten()
    * 입력 이미지의 픽셀 값들을 하나의 행으로 정렬
    * Linear classfier를 이용하여 각 클래스별 점수를 계산
 - PyTorch torch.matmul()
 - PyTorch nn.Linear()
 - 한계점 : 이미지의 일부만 보거나 움직이면 예측 실패 발생 -> 지역적 패턴, 특징 학습을 위해 convolution 도입

#### Convolution operation
 - Convolution : 입력값에 kernel을 곱하는 과정 (kernel : 각 요소별 가중치를 통해 이미지에서 원하는 특징을 추출하기 위한 도구)
 - 용어 : Kernel = filter = weight = window (Convolution 한정)
 - 2D conv. : 이미지에 사용됨
 - Stride : kernel이 입력데이터를 이동하는 간격(1이면 1칸 이동)
 - Pooling : 빠른 계산을 위해 공간 해상도를 줄이기 위해 사용하는 방법
    * Stride를 조절하여 축소 사이즈 결정 (Stride가 n일 때 1/n)
    * 일반적으로 max-pooling(가장 큰 값), average-pooling(평균값) 많이 사용
 - Receptive field : 특정 CNN 필터가 바라보는 입력 공간 상의 영역
    * KxK conv. 필터가 stride가 1이고 pooling이 PxP 이면 (P+K-1)x(P+K-1)만큼을 receptive field로 가짐
 - Hierarchical representation
    * CNN이 학습하는 형태
    * 처음에는 픽셀, 테두리와 같은 단순한 정보 학습 (색깔, 선, 패턴)
    * 점차 복잡하고 자세한 정보 학습 (눈,코,입,귀)
    * 고차원의 정보가 더해져 가장 복잡한 정보 학습 (얼굴 인식)